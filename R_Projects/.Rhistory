mean(movies$Box.Office)
# save data to a CSV file
write.csv(movies, "Movies.csv")
# set wording directory
setwd("C:/Users/sseyl/OneDrive/Documents/GitHub/Data-Science-Projects/R_Projects")
# load CSV data
movies <- read.csv(
file = "Movies.csv",
quote="\""
)
genre <- read.csv(
file = "Genres.csv",
quote="\""
)
# peek at the data
head(movies)
head(genre)
# Univariate statistics for qualitative variables
table(movies$Rating)
table(genre$Genre)
# analyze the location of a quantitative variable
mean(movies$Runtime)
median(movies$Runtime)
which.max(table(movies$Runtime))
min(movies$Runtime)
max(movies$Runtime) # longest movie run time is 219 minutes
range(movies$Runtime)
diff(range(movies$Runtime))
quantile(movies$Runtime)
quantile(movies$Runtime, 0.25)
quantile(movies$Runtime, 0.90)
IQR(movies$Runtime) # interquartile range
var(movies$Runtime)
sd(movies$Runtime) # standard deviation
# analyze the shape of a quantitative variable
install.packages("moments")
library(moments)
skewness(movies$Runtime)
kertosis(movies$Runtime)
kurtosis(movies$Runtime)
plot(density(movies$Runtime))
# summarize a quantitative variable
summary(movies$Runtime)
# Bivariate Statistics for two qualitative variables
table(genres$Genre, genres$Rating)
genres <- read.csv(
file = "Genres.csv",
quote="\""
)
head(genres)
table(genres$Genre) # returns frequency of each genre
table(genres$Genre, genres$Rating)
# bivariate statistics for two quantitative variables
# covariance
cov(movies$Runtime, movies$Box.Office)
cov(movies$Critic.Score, movies$Box.Office)
cor(movies$Runtime, movies$Box.Office)
cor(movies$Critic.Score, movies$Box.Office)
tapply(movies$Box.Office, movies$Rating, mean)
tapply(genres$Box.Office, genres$Genre, mean)
setwd("C:/Users/sseyl/OneDrive/Documents/GitHub/Data-Science-Projects/R_Projects")
# load CSV Files
movies <- read.csv("Movies.csv", quote="\"")
genres <- read.csv("Genres.csv", quote="\"")
# load CSV Files
movies <- read.csv("Movies.csv", quote="\"")
genres <- read.csv("Genres.csv", quote="\"")
head(movies)
head(genres)
plot(movies$Rating)
plot(movies$Rating)
movies$Rating <- as.factor(movies$Rating) # this line is required in newer versions of R, because
plot(movies$Rating)
# create a pie chart of rating observations
pie(table(movies$Rating))
# create a dot plot of runtime
plot(
x = movies$Runtime,
y = rep(0, nrow(movies)), # set y-coord to 0 for all values so that they are on a straight line using rep, repeats 0 once for each row in movies df
ylab="", # set to empty string since there is no dimension set for the y-axis
yaxt = "n")
# create a boxplot of runtime
boxplot(
x = movies$Runtime,
xlab = "Runtime(minutes)",
horizontal = TRUE)
hist(movies$Runtime)
# create a more course-grain histogram
hist(
x = movies$Runtime,
breaks = 10)
# create a more fine-grain histogram
hist(
x = movies$Runtime,
breaks = 30)
# create a density plot of runtime
plot(density(movies$Runtime))
# add dot plot to base of density plot
points(
x = movies$Runtime,
y = rep(-0.0005, nrow(movies))) # set each point to -0.0005 for each row so that it displays just below the y-axis
# create a spine plot
spineplot(
x = genres$Genre,
y = genres$Rating)
genres$Rating <- as.factor(genres$Rating) # w/out this line, we get an error "dependent variable should be a factor"
spineplot(
x = genres$Genre,
y = genres$Rating)
genres$Rating <- as.factor(genres$Rating) # w/out this line, we get an error "dependent variable should be a factor"
spineplot(
x = genres$Genre,
y = genres$Rating)
spineplot(
breaks=NULL,
x = genres$Genre,
y = genres$Rating)
print(genres$Rating)
print(is.factor(genres$Rating))
spineplot(
x = genres$Genre,
y = genres$Rating)
genres$Genre <- as.factor(genres$Genre)
spineplot(
x = genres$Genre,
y = genres$Rating)
genres <- read.csv("Genres.csv", quote="\"")
spineplot(
x = genres$Genre,
y = genres$Rating)
genres$Genre <- as.factor(genres$Genre)
genres$Genre <- as.factor(genres$Genre)
spineplot(
x = genres$Genre,
y = genres$Rating)
genres$Rating <- as.factor(genres$Rating) # w/out this line, we get an error "dependent variable should be a factor"
genres$Genre <- as.factor(genres$Genre)
spineplot(
x = genres$Genre,
y = genres$Rating)
# create a mosaic plot of genre and rating
mosaicplot( # set x to a contigency table that contains both Genre and Rating
x = table( # no y-argument for a mosaic plot
genres$Genre,
genres$Rating),
las = 3) # label access style, rotate the labels
# create a scatterplot of runtime and box office revenue
plot(
x = movies$Runtime,
y = movies$Box.Office)
# Create a scatterplot of critic score and box office revenue
plot(
x = movies$Critic.Score,
y = movies$Box.Office)
# Plot a line of graph of count of movies by year
plot(
x = table(movies$Year),
type = "l")
# create a bar graph of average box office by rating
barplot(tapply(movies$Box.Office, movies$Rating, mean))
# create a bar graph of average box office by genre
barplot(
height = tapply(genres$Box.Office, genres$Genre, mean),
las=3)
# plot bivariate box plots of box office by rating
plot(
x = movies$Rating,
y = movies$Box.Office)
# create a scatterplot matrix
plot(movies)
# clearning up data visualizations
plot(movies$Rating)
# cleaning up the bar chart
plot(
x = movies$Rating,
main = "Count of Movies by Rating",
xlab = "Rating Category",
ylab =  "Count of Movies",
col = "#b3cde3")
?plot
?par
data(iris)
head(iris)
# look at the unique species
unique(iris$Species)
# create a scatterplot matrix
plot(iris[1:4])
# create a scatterplot of petal length vs width
plot(
x = iris$Petal.Length,
y = iris$Petal.Width
)
x <- iris$Petal.Length
y <- iris$Petal.Width
model <-lm(y ~ x)
# draw linear regression model on plot
lines(
x = iris$Petal.Length,
y = model$fitted,
col = "red",
lwd = 3)
# get correlation coefficient
cor(
x = iris$Petal.Length,
y = iris$Petal.Width )
# summarize the model
summary(model)
# predict new unknown values from the model
predict(
object = model,
newdata = data.frame(x = c(2,5,7)))
# load the iris data
data(iris)
# peek at the data
head(iris)
# look at the unique species
unique(iris$Species)
# create a scatterplot matrix colored by species
plot(
x = iris[1:4],
col = as.integer(iris$Species))
# view scatterplot of petal length vs width
plot(
x = iris$Petal.Length,
y = iris$Petal.Width
)
# color scatterplot by species
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
col = as.numeric(iris$Species))
# create k-means cluseters
clusters <- kmeans(
x = iris[, 1:4],
centers = 3, # set to 3 (because we have 3 species)
nstart = 10) # number of algorithm re-starts with different random starting location
# plot each cluster as a shape
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
col = as.numeric(iris$Species),
pch = clusters$cluster)
# plot centroid of clusters
points(
x = clusters$centers[, "Petal.Length"],
y = clusters$centers[, "Petal.Width"],
pch = 4, # plot char to 4 = x char
lwd = 4,
col = "blue")
# view a table of the clusters
table(
x = clusters$cluster,
y = iris$Species)
# load the iris data
data(iris)
# peek at the data
head(iris)
# look at the unique species
unique(iris$Species)
# create a scatterplot matrix colored by species
plot(
x = iris[1:4],
col = as.integer(iris$Species))
# view scatterplot of petal length vs width
plot(
x = iris$Petal.Length,
y = iris$Petal.Width
)
# color scatterplot by species
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
col = as.numeric(iris$Species))
# create k-means cluseters
clusters <- kmeans(
x = iris[, 1:4],
centers = 3, # set to 3 (because we have 3 species)
nstart = 10) # number of algorithm re-starts with different random starting location
# plot each cluster as a shape
# we can see that all of the setosa were correctly identified (black triangles)
# most of the versicolor were predicted correctly (red circles), some are incorrectly identified (green circles)
# most of the virginica were predicted correctly (green crosses), some are incorrectly identified (red crosses)
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
col = as.numeric(iris$Species),
pch = clusters$cluster) # plot character
# plot centroid of clusters
points(
x = clusters$centers[, "Petal.Length"],
y = clusters$centers[, "Petal.Width"],
pch = 4, # plot char to 4 = x char
lwd = 4,
col = "blue")
# view a table of the clusters to show which were correctly identified
# all 50 setosa were correct
# 48 versicolor were correct, 2 incorrect
# 14 virginica were incorrect, 36 correct
table(
x = clusters$cluster,
y = iris$Species)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(reshape2)
housing = read.csv('https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv')
head(housing)
summary(housing)
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms, na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms') # drop these columns
housing = housing[, !(names(housing) %in% drops)]
head(housing)
# get a list of all categories in the ocean_proximity column
categories = unique(housing$ocean_proximity)
# make a new empty dataframe of all 0s where each category becomes a unique column
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
# use a for-loop to populate the appropriate columns of the df
for(cat in categories) {
cat_housing[, cat] = rep(0, times = nrow(cat_housing))
}
head(cat_housing) # see the new column
head(cat_housing)
# drop the original column from the df
for( in in 1:length(cat_housing$ocean_proximity)) {
for(i in 1:length(cat_housing$ocean_proximity)) {
cat = as.character(cat_housing$ocean_proximity[i])
cat_housing[, cat][i] = 1
}
head(cat_housing)
# drop the original column from the df
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing, one_of(keep_columns))
tail(cat_housing)
colnames(housing) # view column names
colnames(housing) # view column names
drops = c('ocean_proxmimity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
colnames(housing) # view column names
drops = c('ocean_proxmimity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)
colnames(housing) # view column names
drops = c('ocean_proxmimity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
colnames(housing) # view column names
drops = c('ocean_proxmimity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
str(housing_num)
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
str(housing_num)
scaled_housing_num = scale(housing_num)
head(scaled_housing_num)
cleaned_housing = cbind(cat_housing, scaled_housing_num, median_house_value=housing$median_house_value)
head(cleaned_housing)
set.seed(1738) # set a random seed so that the same sample can be reproduced in future runs
sample = sample.int(n = nrow(cleaned_housing), size = floor(.8*nrow(cleaned_housing)), replace=F)
train = cleaned_housing[sample, ] # just the sample data
test = cleaned_housing[-sample, ] # everything else
head(train) # peek at the training data
nrow(train) + nrow(test) == nrow(cleaned_housing)
library('boot')
?cv.glm # note the K option for K fold cross validation
glm_house = glm(median_house_value ~ median_income + mean_rooms + population, data = cleaned_housing)
k_fold_cv_error$delta
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse
names(glm_house)
glm_house$coefficients
library('randomForest')
install.packages("randomForest")
library('randomForest')
?randomForest
names(train)
set.seed(1738)
train_y = train[, ' median_house_value']
set.seed(1738)
train_y = train[, 'median_house_value']
train_x = train[, names(train) != 'median_house_value']
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) # examine all of the methods we can call on the model
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) # examine all of the methods we can call on the model
rf_model$importance
rf_model$importance
install.packages("tidyverse")
rf_model$importance
install.packages("tidyverse")
oob_prediction = predict(rf_model)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
library('randomForest') # load library
?randomForest # read about classification and regression with random Forest
names(train) # view column names in training data
set.seed(1738)
train_y = train[, 'median_house_value']
train_x = train[, names(train) != 'median_house_value']
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) # examine all of the methods we can call on the model
rf_model$importance
# out-of-bag error estimate
oob_prediction = predict(rf_model)
oob_rmse = sqrt(train_mse)
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
oob_rmse
# test data
test_y = test[,'median_house_value']
text_x = test[, names(test) != 'median_house_value']
y_pred = predict(rf_model, test_x)
# test data
test_y = test[,'median_house_value']
test_x = test[, names(test) != 'median_house_value']
y_pred = predict(rf_model, test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
glm_house1 = glm(median_house_value ~ median_income + total_bedrooms + longitude, latitude, data = cleaned_housing) # fit the data
glm_house1 = glm(median_house_value ~ median_income + total_bedrooms + longitude + latitude, data = cleaned_housing) # fit the data
glm_house1 = glm(median_house_value ~ median_income + longitude + latitude, data = cleaned_housing) # fit the data
glm_houseMod = glm(median_house_value ~ median_income + longitude + latitude, data = cleaned_housing) # fit the data
# cleaned_housing data, fitted data, K = 5: training data split into 5 equal portions
# one of the five folds is put off to the side as a mini test set, model is trained on the remaining 4 folds
# The process is repeated on each of the 5 folds and the average predictions produced from the iterations fo the model is taken
k_fold_cv_error = cv.glm(cleaned_housing, glm_houseMod, K=5)
library('boot') # load library
glm_houseMod = glm(median_house_value ~ median_income + longitude + latitude, data = cleaned_housing) # fit the data
# cleaned_housing data, fitted data, K = 5: training data split into 5 equal portions
# one of the five folds is put off to the side as a mini test set, model is trained on the remaining 4 folds
# The process is repeated on each of the 5 folds and the average predictions produced from the iterations fo the model is taken
k_fold_cv_error = cv.glm(cleaned_housing, glm_houseMod, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse # off by about $83,629
names(glm_houseMod) # these are the methods we can use on the model
glm_houseMod$coefficients # Intercept 206855.814, median income: $82608.959, mean rooms: -9755.442, population: -3948.293
glm_houseMod = glm(median_house_value ~ median_income + longitude + latitude, data = cleaned_housing) # fit the data
# cleaned_housing data, fitted data, K = 5: training data split into 5 equal portions
# one of the five folds is put off to the side as a mini test set, model is trained on the remaining 4 folds
# The process is repeated on each of the 5 folds and the average predictions produced from the iterations fo the model is taken
k_fold_cv_error = cv.glm(cleaned_housing, glm_houseMod, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse # off by about $74k
names(glm_houseMod) # these are the methods we can use on the model
glm_houseMod$coefficients # Intercept 206855.814, median income: $69624.10, longitude: -99947.36, latitude: -103496.59
names(train) # view column names in training data
set.seed(1738)
train_y = train[, 'median_house_value']
train_x = train[, names(train) != 'median_house_value']
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) # examine all of the methods we can call on the model
rf_model$importance
# out-of-bag error estimate
oob_prediction = predict(rf_model)
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse # using a random forest of 1000 decision trees, we are able to predict the mdian price of a house in a given district to w/in $49k of actual median price
# test data
test_y = test[,'median_house_value']
test_x = test[, names(test) != 'median_house_value']
y_pred = predict(rf_model, test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse # $47625.57, pretty close to training data
names(train) # view column names in training data
set.seed(1738)
train_y = train[, 'median_house_value']
train_x = train[, names(train) != 'median_house_value']
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)
names(rf_model) # examine all of the methods we can call on the model
rf_model$importance
# out-of-bag error estimate
oob_prediction = predict(rf_model)
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse # using a random forest of 1000 decision trees, we are able to predict the mdian price of a house in a given district to w/in $49k of actual median price
# test data
test_y = test[,'median_house_value']
test_x = test[, names(test) != 'median_house_value']
y_pred = predict(rf_model, test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse # $47625.57, pretty close to training data
