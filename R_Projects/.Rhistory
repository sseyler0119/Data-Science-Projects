install.packages("dplyr")
install.packages("rlang")
install.packages("dplyr")
install.packages(c("lifecycle", "pillar", "tidyselect"))
library(dplyr)
install.packages("dplyr")
install.packages("rlang")
library(dplyr)
install.packages("assertthat")
install.packages("~/CS Courses/Data Science/dplyr_1.0.10.tar.gz", repos = NULL, type = "source")
# Predicting with Machine Learning
# load the data
data(iris)
# set a seed to make randomness reproducable
set.seed(42)
# split into training and test data set
# randomly sample 100 of 150 row indexes, training data
indexes <- sample(
x = 1:150,
size = 100)
# inspect the random indexes
print(indexes)
# create a training set from indexes
train <- iris[indexes,]
# create a test set from remaining indexes
test <- iris[-indexes, ] # -indexes says to include all rows except those listed in the row indexes variable (the remaining data)
# load the decision tree package
library(tree)
# train a decision tree model
#model <- tree(
# formula = Species ~ ., # . says to include all other variables in the data frame as the explanatory variable
#data = train) # Species ~ . is equivalent to formula = Species ~ Petal.Length + Petal.Width + Sepal.Length + Sepal.Width
# use two predictors, using the above model to train the decision tree causes an error with
#     the partition.tree() command, "tree can only have one or two predictors", this is the workaround
model <- tree(
formula = Species ~ Petal.Length + Petal.Width,
data = train)
# inspect the model
summary(model)
# visualize the decision tree model
plot(model)
text(model)
# Load color brewer library
library(RColorBrewer)
# create color palette
palette <- brewer.pal(3, "Set2") # color blind friendly
# Create a scatterplot colored by species
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
pch = 19,
col = palette[as.numeric(iris$Species)],
main = "Iris Petal Length vs. Width",
xlab = "Petal Length (cm)",
ylab = "Petal Width (cm)")
# plot the decision tree boundaries
partition.tree(
tree = model,
label = "Species",
add = TRUE)
# predict with the model
predictions <- predict(
object = model,
newdata = test,
type = "class")
# create a confusion matrix
table(
x = predictions,
y = test$Species)
# load the caret package
library(ggplot2)
library(caret)
# evaluate the prediction results
confusionMatrix(
data = predictions,
reference = test$Species)
# Predicting with Machine Learning
# load the data
data(iris)
# set a seed to make randomness reproducable
set.seed(42)
# split into training and test data set
# randomly sample 100 of 150 row indexes, training data
indexes <- sample(
x = 1:150,
size = 100)
# inspect the random indexes
print(indexes)
# create a training set from indexes
train <- iris[indexes,]
# create a test set from remaining indexes
test <- iris[-indexes, ] # -indexes says to include all rows except those listed in the row indexes variable (the remaining data)
# load the decision tree package
library(tree)
# train a decision tree model
#model <- tree(
# formula = Species ~ ., # . says to include all other variables in the data frame as the explanatory variable
#data = train) # Species ~ . is equivalent to formula = Species ~ Petal.Length + Petal.Width + Sepal.Length + Sepal.Width
# use two predictors, using the above model to train the decision tree causes an error with
#     the partition.tree() command, "tree can only have one or two predictors", this is the workaround
model <- tree(
formula = Species ~ Petal.Length + Petal.Width,
data = train)
# inspect the model
summary(model)
# visualize the decision tree model
plot(model)
text(model)
# Load color brewer library
library(RColorBrewer)
# create color palette
palette <- brewer.pal(3, "Set2") # color blind friendly
# Create a scatterplot colored by species
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
pch = 19,
col = palette[as.numeric(iris$Species)],
main = "Iris Petal Length vs. Width",
xlab = "Petal Length (cm)",
ylab = "Petal Width (cm)")
# plot the decision tree boundaries
partition.tree(
tree = model,
label = "Species",
add = TRUE)
# predict with the model
predictions <- predict(
object = model,
newdata = test,
type = "class")
# create a confusion matrix
table(
x = predictions,
y = test$Species)
# load the caret package
library(ggplot2)
library(caret)
# evaluate the prediction results
confusionMatrix(
data = predictions,
reference = test$Species)
install.packages("ggplot2")
library(ggplot2)
install.packages("dplyr")
setwd("C:/Users/sseyl/OneDrive/Documents/GitHub/Data-Science-Projects/R_Projects") # use / instead of \ for directory
# read in a CSV data file
cars <- read.csv("Cars.csv")
# load library
library(ggplot2)
# set the working directory
setwd("C:/Users/sseyl/OneDrive/Documents/GitHub/Data-Science-Projects/R_Projects") # use / instead of \ for directory
library(ff)
install.packages("ff")
library(ff)
# read a csv file as ff data frame
irisff <- read.table.ffdf( # creates a pointer to the data frame
file = "Iris.csv",
FUN = "read.csv")
# inspect the class
class(irisff) # returns object of type ffdf instead of the data frame
# inspect the column names
names(irisff)
# inspect the first few rows
irisff[1:5,] # can't use head function, need to use indexing operators
# load the biglm package
library(biglm)
install.packages("biglm")
library(biglm)
# create linear regression model
model <- biglm(
formula = Petal.Width ~ Petal.Length,
data = irisff)
# summarize the model
summary(model)
# create a scatterplot
plot(
x = iris$Petal.Length[], # [] required because this is just a pointer to the data, not the data itself
y = iris$Petal.Width[],
main = "Iris Petal Length vs Width",
xlab = "Petal Length (cm)",
ylab = "Petal Width (cm)")
# get the y-intercept from model
b<- summary(model)$mat[1,1]
# get slope from model
m <- summary(model)$mat[2,1]
lines(
x = irisff$Petal.Length[],
y = m * irisff$Petal.Length[] + b,
col = "red",
lwd = 3)
# predict new values with the model
predict(
object = model,
newdata = data.frame(
Petal.Length = c(2, 5, 7),
Petal.Width = c(0, 0, 0)))
setwd("C:/Users/sseyl/OneDrive/Documents/GitHub/Data-Science-Projects/R_Projects") # use / instead of \ for directory
# Load the ff package
library(ff)
# Read a CSV file as ff data frame
irisff <- read.table.ffdf(
file = "Iris.csv",
FUN = "read.csv")
# inspect the class
class(irisff)
# inspect the column names
names(irisff)
irisff[1:5,] # can't use head since this is a df
library(biglm)
# Create a linear regression model
model <- biglm(
formula = Petal.Width ~ Petal.Length,
data = irisff)
# Summarize the model
summary(model)
# Draw a regression line on plot
lines(
x = irisff$Petal.Length[],
y = m * irisff$Petal.Length[] + b,
col = "red",
lwd = 3)
# Predict new values with the model
predict(
object = model,
newdata = data.frame(
Petal.Length = c(2, 5, 7),
Petal.Width = c(0, 0, 0)))
# Predicting with Machine Learning
# load the data
data(iris)
# set a seed to make randomness reproducable
set.seed(42)
# split into training and test data set
# randomly sample 100 of 150 row indexes, training data
indexes <- sample(
x = 1:150,
size = 100)
# inspect the random indexes
print(indexes)
# create a training set from indexes
train <- iris[indexes,]
# create a test set from remaining indexes
test <- iris[-indexes, ] # -indexes says to include all rows except those listed in the row indexes variable (the remaining data)
# load the decision tree package
library(tree)
install.packages("tree")
# Predicting with Machine Learning
# load the data
data(iris)
# set a seed to make randomness reproducable
set.seed(42)
# split into training and test data set
# randomly sample 100 of 150 row indexes, training data
indexes <- sample(
x = 1:150,
size = 100)
# inspect the random indexes
print(indexes)
# create a training set from indexes
train <- iris[indexes,]
# create a test set from remaining indexes
test <- iris[-indexes, ] # -indexes says to include all rows except those listed in the row indexes variable (the remaining data)
# load the decision tree package
library(tree)
# train a decision tree model
#model <- tree(
# formula = Species ~ ., # . says to include all other variables in the data frame as the explanatory variable
#data = train) # Species ~ . is equivalent to formula = Species ~ Petal.Length + Petal.Width + Sepal.Length + Sepal.Width
# use two predictors, using the above model to train the decision tree causes an error with
#     the partition.tree() command, "tree can only have one or two predictors", this is the workaround
model <- tree(
formula = Species ~ Petal.Length + Petal.Width,
data = train)
# inspect the model
summary(model)
# visualize the decision tree model
plot(model)
text(model)
# Load color brewer library
library(RColorBrewer)
# create color palette
palette <- brewer.pal(3, "Set2") # color blind friendly
# Create a scatterplot colored by species
plot(
x = iris$Petal.Length,
y = iris$Petal.Width,
pch = 19,
col = palette[as.numeric(iris$Species)],
main = "Iris Petal Length vs. Width",
xlab = "Petal Length (cm)",
ylab = "Petal Width (cm)")
# plot the decision tree boundaries
partition.tree(
tree = model,
label = "Species",
add = TRUE)
# predict with the model
predictions <- predict(
object = model,
newdata = test,
type = "class")
# create a confusion matrix
table(
x = predictions,
y = test$Species)
install.packages("caret")
library(caret)
install.packages("ggplot2")
